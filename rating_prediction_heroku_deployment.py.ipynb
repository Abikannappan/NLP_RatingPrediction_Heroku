{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "***Rating Prediction Heroku Deployment***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** Importing libraries *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:36:44.222600Z",
     "start_time": "2019-12-04T07:36:33.333350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abinl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:36:46.831615Z",
     "start_time": "2019-12-04T07:36:44.222600Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the csv Appliances.csv file as a dataframe\n",
    "df=pd.read_csv('./df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'asin', 'title', 'tech1', 'reviewText', 'overall',\n",
       "       'reviewerID', 'reviewTime', 'year', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:36:51.738458Z",
     "start_time": "2019-12-04T07:36:51.720224Z"
    }
   },
   "outputs": [],
   "source": [
    "# describing our own stopwords to avoid some of the negative words like 'not' from being stopped\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "              \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
    "              'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "              'themselves',  'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is',\n",
    "              'are', 'was', 'were', 'be', 'been', 'being',  'a', 'an', 'the', 'and',  'or', 'as','for','of','all','would',\n",
    "               'at', 'by', 'through', 'to', 'from', 'in', 'out','on', 'off',  'then', 'here','there', 'when', 'one',\n",
    "              'where', 'why', 'how','so',   's', 't',   'd', 'll', 'm', 'o','re', 've','y', 'ain','if',\n",
    "               'ma','what', 'aa','will','name','have','with','go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:36:51.758647Z",
     "start_time": "2019-12-04T07:36:51.738458Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to do initial cleaning by setting regex to alphanumeric, changing to lower case and to remove stop words\n",
    "def clean_round1(text):\n",
    "# [A-Za-z]+ this regex finds the words \n",
    "# The result of text is a list    \n",
    "    text = re.findall('[A-Za-z]+',text)\n",
    "# convert all the text collected from regex into lower case in a list using list comprehension\n",
    "    text = [x.lower() for x in text]\n",
    "# write all words other than stop words into a list\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "# converting list into string    \n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:36:52.853502Z",
     "start_time": "2019-12-04T07:36:51.760677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the cleaning function to the reviewText\n",
    "df['cleanText']=df['reviewText'].apply(lambda x: clean_round1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:40:11.859335Z",
     "start_time": "2019-12-04T07:36:52.853502Z"
    }
   },
   "outputs": [],
   "source": [
    "# lemmatized text using spacy. stemming is not done as some cell will only have null values after stemming\n",
    "nlp =spacy.load('en_core_web_sm')\n",
    "df['lemmatext']=df['cleanText'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:40:12.701451Z",
     "start_time": "2019-12-04T07:40:11.866172Z"
    }
   },
   "outputs": [],
   "source": [
    "# stemming is done to clean the data and reduce features, which will reduce the complexity of the model\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "df['stemtext'] = [ps.stem(x) for x in df['lemmatext']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:40:16.659858Z",
     "start_time": "2019-12-04T07:40:14.707657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding using count vectorizer. Bigrams and Trigrams are created to feed meaningful features to the model\n",
    "cvec = CountVectorizer(stop_words=stop_words,max_features=6000,ngram_range=(1, 3))\n",
    "\n",
    "# Fit our vectorizer on the lemmatized data\n",
    "cvec.fit(df['stemtext'])\n",
    "\n",
    "# and check out the length of the vectorized data after\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cvec, open('transform.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:40:16.677810Z",
     "start_time": "2019-12-04T07:40:16.662850Z"
    }
   },
   "outputs": [],
   "source": [
    "# index is changed after vectorization. so index before vectorisation is saved as a list \n",
    "# and to used in the dataframe after vectorization.\n",
    "index = []\n",
    "for row in df['stemtext'].index: \n",
    "    index.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:40:22.196471Z",
     "start_time": "2019-12-04T07:40:19.765097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able buy</th>\n",
       "      <th>able find</th>\n",
       "      <th>able get</th>\n",
       "      <th>about</th>\n",
       "      <th>about buy</th>\n",
       "      <th>about cheap</th>\n",
       "      <th>about day</th>\n",
       "      <th>about every</th>\n",
       "      <th>...</th>\n",
       "      <th>year use</th>\n",
       "      <th>year work</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet but</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  able buy  able find  able get  about  about buy  \\\n",
       "0        0     0         0          0         0      0          0   \n",
       "1        0     0         0          0         0      0          0   \n",
       "2        0     0         0          0         0      0          0   \n",
       "3        0     0         0          0         0      1          0   \n",
       "4        0     0         0          0         0      0          0   \n",
       "\n",
       "   about cheap  about day  about every  ...  year use  year work  yep  yes  \\\n",
       "0            0          0            0  ...         0          0    0    0   \n",
       "1            0          0            0  ...         0          0    0    0   \n",
       "2            0          0            0  ...         0          0    0    0   \n",
       "3            0          0            0  ...         0          0    0    0   \n",
       "4            0          0            0  ...         0          0    0    0   \n",
       "\n",
       "   yesterday  yet  yet but  youtube  zero  zero water  \n",
       "0          0    0        0        0     0           0  \n",
       "1          0    0        0        0     0           0  \n",
       "2          0    0        0        0     0           0  \n",
       "3          0    0        0        0     0           0  \n",
       "4          0    0        0        0     0           0  \n",
       "\n",
       "[5 rows x 6000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe created with vectorized stemtext column using count vectorizer.\n",
    "X_cvec = pd.DataFrame(cvec.transform(df['stemtext']).todense(),\n",
    "                       columns=cvec.get_feature_names(),index=index)\n",
    "X_cvec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T04:13:23.324106Z",
     "start_time": "2019-11-17T04:13:23.320140Z"
    }
   },
   "source": [
    "## TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:42:51.553723Z",
     "start_time": "2019-12-04T07:42:51.522020Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abinl\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver ='liblinear')\n",
    "model = lr.fit(X_cvec, df['overall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nlp_model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr,open(filename,'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
